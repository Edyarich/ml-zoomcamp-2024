Extracted Cells:
\Cell 1:
----------------------------------------
# Homework #2: train a CTC speech recognition model

In lecture you have examined the basics of speech recognition and covered the Connectionist Temporal Classification (CTC) model in detail. You are now ready to train your first "adult" speech recognition system!

In seminar 2 you implemented the CTC forward and backward algorithms in order to calculate the CTC loss and study the diffusion of probability in a CTC trellis. Also you implemented a greedy decoder and a prefix beam-search decoder

In this homework you will implement and train a CTC speech recognition model on a subset of the LibriSpeech corpus. This task will involve:

- Creating a dataloader
- Implementing a and training a Neural Network for CTC
  * DNN
  * LSTM
  * BiLSTM
- Comparing the Properties of these models
\Cell 2:
----------------------------------------
# Setup - Install package, download files, etc...
\Cell 3:
----------------------------------------
# uncomment if needed. If you run the notebook in Colab, all these libraries are pre-installed
# !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
# !pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0
\Cell 4:
----------------------------------------
# !pip install arpa
\Cell 5:
----------------------------------------
%%capture
# !pip install wandb -qqq
\Cell 6:
----------------------------------------
#!L
import math
import os
import gc
import shutil
import string
import time
from collections import defaultdict
from typing import List, Tuple, TypeVar, Optional, Callable, Iterable

import arpa
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchaudio
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import torchaudio.transforms as tt
import wandb
from matplotlib.colors import LogNorm
from torch import optim
from tqdm.notebook import tqdm
\Cell 7:
----------------------------------------
import utils as utils   # Change relative path if needed
\Cell 8:
----------------------------------------
# Seminar 2 recap: CTC Forward-Backward Algorithm + Soft alignment
\Cell 9:
----------------------------------------
## CTC Forward Algorithm
\Cell 10:
----------------------------------------
# Helper functions
BLANK_SYMBOL = "_"

class Tokenizer:
    """
    Maps characters to integers and vice versa
    """
    def __init__(self):
        self.char_map = {}
        self.index_map = {}
        for i, ch in enumerate(["'", " "] + list(string.ascii_lowercase) + [BLANK_SYMBOL]):
            self.char_map[ch] = i
            self.index_map[i] = ch
        
    def text_to_indices(self, text: str) -> List[int]:
        return [self.char_map[ch] for ch in text]

    def indices_to_text(self, labels: List[int]) -> str:                                                                                                                                                                                                                                 
        return "".join([self.index_map[i] for i in labels])
    
    def get_symbol_index(self, sym: str) -> int:
        return self.char_map[sym]
    

tokenizer = Tokenizer()

NEG_INF = -float("inf")


def logsumexp(*args) -> float:
    """
    Log-sum-exp trick for log-domain calculations
    See for details: https://en.wikipedia.org/wiki/LogSumExp
    """
    if all(a == NEG_INF for a in args):
        return NEG_INF
    a_max = max(args)
    lsp = math.log(sum(math.exp(a - a_max) for a in args))
    return a_max + lsp


def modify_sequence(sequence: List[int], blank_idx: int) -> List[int]:
    """
    Modifies sequence which with START, END blanks and between each character
    """
    modified_sequence = []
    
    for idx in sequence:
        modified_sequence += [blank_idx, idx]
        
    modified_sequence.append(blank_idx)
    return modified_sequence
\Cell 11:
----------------------------------------
#!L

def forward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:
    """
    :param sequence: a string converted to an index array by Tokenizer
    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.
    :return: the result of the forward pass of shape (2 * len(sequence) + 1, T)
    """
    # Turn probs into log-probs
    matrix = np.log(matrix)
    
    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)
    mod_sequence = modify_sequence(sequence, blank)

    # Initialze
    # (2L + 1) x T 
    alphas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)

    for t in range(matrix.shape[1]):
        for s in range(len(mod_sequence)):
            # First Step
            ch = mod_sequence[s]
            if t == 0:
                if s != 0 and s != 1:
                    alphas[s][t] = NEG_INF
                else:
                    alphas[s][t] = matrix[ch][t]
                
            # Upper diagonal zeros
            elif s < alphas.shape[0] - 2 * (alphas.shape[1]-t)-1:# CONDITION
                alphas[s][t] = NEG_INF
            else:
                # Need to do this stabily
                if s == 0:
                    alphas[s][t] = alphas[s][t-1] + matrix[ch][t]
                elif s == 1:
                    alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]
                else:
                    if ch == blank or ch == mod_sequence[s-2]:
                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1]) + matrix[ch][t]
                    else:
                        alphas[s][t] = logsumexp(alphas[s][t-1], alphas[s-1][t-1], alphas[s-2][t-1]) + matrix[ch][t]
    return alphas
\Cell 12:
----------------------------------------
## The CTC Backward Algorithm
\Cell 13:
----------------------------------------
def backward_algorithm(sequence: List[int], matrix: np.ndarray) -> np.ndarray:
    """
    :param sequence: a string converted to an index array by Tokenizer
    :param matrix: A matrix of shape (K, T) with probability distributions over phonemes at each moment of time.
    :return: the result of the backward pass of shape (2 * len(sequence) + 1, T)
    """
    matrix = np.log(matrix)
    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)
    mod_sequence = modify_sequence(sequence, blank)
    betas = np.full([len(mod_sequence), matrix.shape[1]], NEG_INF)

    for t in reversed(range(matrix.shape[1])):
        for s in reversed(range(len(mod_sequence))):
            # First Step
            ch = mod_sequence[s]
            if t == matrix.shape[1] - 1:
                if s == betas.shape[0]-1 or s == betas.shape[0]-2:
                    betas[s][t] = 0

            # Lower Diagonal Zeros
            elif s > 2 * t + 1:# CONDITION
                betas[s][t] = NEG_INF
            else:
                if s == len(mod_sequence) - 1:
                    betas[s][t] = betas[s][t+1] + matrix[ch][t]
                elif s == len(mod_sequence) - 2:
                    betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]
                else:
                    if ch == blank or ch == mod_sequence[s + 2]:
                            betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1]) + matrix[ch][t]
                    else:                
                        betas[s][t] = logsumexp(betas[s][t+1], betas[s+1][t+1], betas[s+2][t+1]) + matrix[ch][t]
    return betas
\Cell 14:
----------------------------------------
## Soft-Alignment

\Cell 15:
----------------------------------------
def soft_alignment(labels_indices: List[int], matrix: np.ndarray) -> np.ndarray:
    """
    Returns the alignment coefficients for the input sequence
    """
    alphas = forward_algorithm(labels_indices, matrix)
    betas = backward_algorithm(labels_indices, matrix)

    # Move from log space back to prob space
    align = np.exp(alphas + betas)

    # Normalize Alignment
    align = align / np.sum(align, axis=0)

    return align
\Cell 16:
----------------------------------------
## Greedy Best-Path Decoder

\Cell 17:
----------------------------------------
#!L
def greedy_decoder(output: torch.Tensor, labels: List[torch.Tensor], 
                   label_lengths: List[int], collapse_repeated: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """
    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]
    :param labels: list of label indices converted to torch.Tensors of shape (batch, time)
    :param label_lengths: list of label lengths (without padding)
    :param collapse_repeated: whether the repeated characters should be deduplicated
    :return: the result of the decoding and the target sequence
    """
    blank_label = tokenizer.get_symbol_index(BLANK_SYMBOL)

    # Get max classes
    ########################
    arg_maxes = output.argmax(dim=-1)
    ########################

    decodes = []
    targets = []

    # For targets and decodes remove repeats and blanks
    for i, args in enumerate(arg_maxes):
        decode = []
        true_labels = labels[i][:label_lengths[i]].tolist()
        targets.append(tokenizer.indices_to_text(true_labels))

        # Remove repeats, then remove blanks
        for j, index in enumerate(args):
            ########################
            if j != 0:
                if index == args[j-1]:
                    continue
            decode.append(int(index.cpu().detach()))    
            ########################
        ####
        decode = [x for x in decode if x != blank_label]
        ######
        
        decodes.append(tokenizer.indices_to_text(decode))
    return decodes, targets
\Cell 18:
----------------------------------------
## Prefix Decoding With LM
\Cell 19:
----------------------------------------
LanguageModel = TypeVar("LanguageModel")
# Helper function

class Beam:
    def __init__(self, beam_size: int) -> None:
        self.beam_size = beam_size
        
        fn = lambda : (NEG_INF, NEG_INF)
        
        # Store probs key - prefix, value - p_blank, p_not_blank for ? step
        self.candidates = defaultdict(fn)
        
        # Store sorted by cumulative probability self.candidates
        self.top_candidates_list = [
            (
                tuple(), 
                (0.0, NEG_INF) # log(p_blank) = 0, log(p_not_blank) = -inf
            )
        ]
        
    def get_probs_for_prefix(self, prefix: Tuple[int]) -> Tuple[float, float]:
        p_blank, p_not_blank = self.candidates[prefix]
        return p_blank, p_not_blank
        
    def update_probs_for_prefix(self, prefix: Tuple[int], next_p_blank: float, next_p_not_blank: float) -> None:
        self.candidates[prefix] = (next_p_blank, next_p_not_blank)
        
    def update_top_candidates_list(self) -> None:
        top_candidates = sorted(
            self.candidates.items(), 
            key=lambda x: logsumexp(*x[1]), 
            reverse=True
        )
        self.top_candidates_list = top_candidates[:self.beam_size]
        

def calculate_probability_score_with_lm(lm: LanguageModel, prefix: str) -> float:
    text = tokenizer.indices_to_text(prefix).upper().strip()    # Use upper case for LM and remove the trailing space
    lm_prob = lm.log_p(text)             
    score = lm_prob / np.log10(np.e)    # Convert to natural log, as ARPA LM uses log10   
    return score
\Cell 20:
----------------------------------------
#!L

def decode(probs: np.ndarray, beam_size: int = 5, lm: Optional[LanguageModel] = None, 
           prune: float = 1e-5, alpha: float = 0.1, beta: float = 2):
    """
    :param probs: A matrix of shape (T, K) with probability distributions over phonemes at each moment of time.
    :param beam_size: the size of beams
    :lm: arpa language model
    :prune: the minimal probability for a symbol at which it can be added to a prefix
    :alpha: the parameter to de-weight the LM probability
    :beta: the parameter to up-weight the length correction term
    :return: the prefix with the highest sum of probabilites P_blank and P_not_blank
    """
    T, S = probs.shape
    probs = np.log(probs)
    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)
    space = tokenizer.get_symbol_index(" ")
    prune = NEG_INF if prune == 0.0 else np.log(prune)
    
    beam = Beam(beam_size)
    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏
    for t in range(T):
        next_beam = Beam(beam_size)
        
        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for s in range(S):
            p = probs[t, s]
            # Prune the vocab - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏–º–≤–æ–ª, –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ–∫–∞–∑–∞—Ç—å—Å—è –≤ –Ω–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞ –Ω–∞ t-–º —â–∞–≥–µ
            if p < prune:   
                continue
            
            # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤–∞—Ä–∏–Ω–∞—Ç–∞–º, –≤ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ–º –ø–æ–π—Ç–∏ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞
            # –°–Ω–∞—á–∞–ª–∞ –∏–¥—É—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø–æ —Å—É–º–º–µ log(p_blank + p_not_blank) –ø—Ä–µ—Ñ–∏–∫—Å—ã
            # (p_blank, p_not_blank) - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º t-1 —à–∞–≥–µ
            for prefix, (p_blank, p_not_blank) in beam.top_candidates_list:
                # –¢–µ–∫—É—â–∏–π —Å–∏–º–≤–æ–ª - –±–ª–∞–Ω–∫ 
                if s == blank:
                    # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ
                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)
                    next_beam.update_probs_for_prefix(
                        prefix=prefix,
                        next_p_blank=logsumexp(p_b, p_blank + p, p_not_blank + p),
                        next_p_not_blank=p_nb
                    )
                    continue

                end_t = prefix[-1] if prefix else None
                n_prefix = prefix + (s,)
                
                # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Å–∏–º–≤–æ–ª
                if s == end_t:
                    # –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–∏–º–≤–æ–ª - –±–ª–∞–Ω–∫
                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)
                    next_beam.update_probs_for_prefix(
                        prefix=n_prefix,
                        next_p_blank=p_b,
                        next_p_not_blank=logsumexp(p_nb, p + p_blank)
                    )
                    # –ü—Ä–µ–¥—É–¥—â–∏–π —Å–∏–º–≤–æ–ª –Ω–µ –±–ª–∞–Ω–∫
                    p_b, p_nb = next_beam.get_probs_for_prefix(prefix)
                    next_beam.update_probs_for_prefix(
                        prefix=prefix,
                        next_p_blank=p_b,
                        next_p_not_blank=logsumexp(p_nb, p + p_not_blank)
                    )
                elif s == space and end_t is not None and lm is not None:
                    # –°–∏–º–≤–æ–ª - –ø—Ä–æ–±–µ–ª –∏ –Ω–µ –ø–µ—Ä–≤—ã–π, –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å
                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)
                    score = calculate_probability_score_with_lm(lm, n_prefix)
                    length = len(tokenizer.indices_to_text(prefix))
                    
                    next_beam.update_probs_for_prefix(
                        prefix=n_prefix,         
                        next_p_blank=p_b,
                        next_p_not_blank=logsumexp(
                            p_nb,
                            p_blank + p + score * alpha + np.log(length) * beta,
                            p_not_blank + p + score * alpha + np.log(length) * beta
                        )  
                    )
                else:
                    p_b, p_nb = next_beam.get_probs_for_prefix(n_prefix)
                    next_beam.update_probs_for_prefix(
                        prefix=n_prefix,
                        next_p_blank=p_b,
                        next_p_not_blank=logsumexp(p_nb, p_blank + p, p_not_blank + p)
                    )

        next_beam.update_top_candidates_list()
        beam = next_beam

    best = beam.top_candidates_list[0]
    return best[0], -logsumexp(*best[1])


def beam_search_decoder(probs: np.ndarray, labels: List[List[int]], label_lengths: List[int], 
                        input_lengths: List[int], lm: LanguageModel, beam_size: int = 5,
                        prune: float = 1e-3, alpha: float = 0.1, beta: float = 0.1):
    probs = probs.cpu().detach().numpy()
    decodes, targets = [], []
    
    for i, prob in enumerate(probs):
        targets.append(tokenizer.indices_to_text(labels[i][:label_lengths[i]].tolist()))
        int_seq, _ = decode(prob[:input_lengths[i]], lm=lm, beam_size=beam_size, prune=prune, alpha=alpha, beta=beta)
        decodes.append(tokenizer.indices_to_text(int_seq))
        
    return decodes, targets
\Cell 21:
----------------------------------------
# Homework 2 starts here: CTC Speech Recognition System
You can do this notebook in google collab, or use other GPU sources

### Tasks

- (25 points) Train ASR System, WER criterions: 60-50 -- 9 points, 50-40 -- 15 points, 40-35 -- 20 points, <=35 -- 25 points. + 1 bonus point per 1% WER below 30
- (2 points) Compare performance of DNN, RNN and BiRNN models in terms of WER, training time and other properties
- (3 points) Compare alignments obtained from DNN, RNN and BiRNN models
\Cell 22:
----------------------------------------
## Implementing, training and evaluationg your CTC ASR model
\Cell 23:
----------------------------------------
### Create a Dataloader

The first step is to create a dataloader to download and load and preprocess LibriSpeech acoustic data. 

The creative options you have at this stage are:

* The sample rate and number of mel-bins.
* Various forms of data agumentation
\Cell 24:
----------------------------------------
#!L
# Download LibriSpeech 100hr training and test data

if not os.path.isdir("./data"):
    os.makedirs("./data")

train_dataset = torchaudio.datasets.LIBRISPEECH("./data", url="train-clean-100", download=True)
test_dataset = torchaudio.datasets.LIBRISPEECH("./data", url="test-clean", download=True)
\Cell 25:
----------------------------------------
#!L
train_audio_transforms = nn.Sequential(
    tt.MelSpectrogram(),
    tt.TimeMasking(time_mask_param=15),
    tt.FrequencyMasking(freq_mask_param=20)
)

test_audio_transforms = nn.Sequential(
    tt.MelSpectrogram()
)
\Cell 26:
----------------------------------------
class Collate:
    def __init__(self, data_type = 'test') -> None:
        super(Collate, self).__init__() 

        self.data_type = data_type

    def __call__(self, data: torchaudio.datasets.librispeech.LIBRISPEECH) -> Tuple[List[torch.Tensor], ...]:
        """
        :param data: is a list of tuples of [features, label], where features has dimensions [n_features, length]
        "returns features, lengths, labels: 
              features is a Tensor [batchsize, features, max_length]
              lengths is a Tensor of lengths [batchsize]
              labels is a Tesnor of targets [batchsize]
        """

        spectrograms = []
        labels = []
        input_lengths = []
        label_lengths = []
        for (waveform, _, utterance, _, _, _) in data:
            if self.data_type == 'train':
                spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)
            elif self.data_type == 'test':
                spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)
            else:
                raise Exception('data_type should be train or valid')
            spectrograms.append(spec)
            label = torch.Tensor(tokenizer.text_to_indices(utterance.lower()))
            labels.append(label)
            input_lengths.append(spec.shape[0] // 2)
            label_lengths.append(len(label))

        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)
        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)

        return spectrograms, labels, input_lengths, label_lengths

\Cell 27:
----------------------------------------
### Implement a Neural Network Model

You should try out a few different model types:
- Feed-Forward Model (DNN)
- Recurrent Model (GRU or LSTM)
- Bidirectional Recurrent Model (bi-GRU or bi-LSTM)
- Something different for bonus points

Before any of this models you can use convolutional layers, as shown in the example below

After your experiments you should write a report with comparison of different models in terms of different features, for example: parameters, training speed, resulting quality, spectrogram properties, and data augmentations. Remember, that for full mark you need to achive good WER 

WER criterions: 60-50 -- 9 points, 50-40 -- 15 points, 40-35 -- 20 points, <= 35 -- 25 points
\Cell 28:
----------------------------------------
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):
        super(DepthwiseSeparableConv, self).__init__()
        
        # Depthwise convolution: groups=in_channels makes it perform a separate convolution for each input channel
        self.depthwise = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=in_channels,
        )
        
        # Pointwise convolution: 1x1 convolution to mix the channels
        self.pointwise = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=1,
            stride=1,
            padding=0,
        )
    
    def forward(self, x):
        out = self.depthwise(x)
        out = self.pointwise(out)
        return out

\Cell 29:
----------------------------------------
#!L

# Our model classes are just examples, you can change them as you want

# Define model
class CNNLayerNorm(nn.Module):
    """Layer normalization built for CNNs input"""

    def __init__(self, n_feats: int) -> None:
        super(CNNLayerNorm, self).__init__()
        self.layer_norm = nn.LayerNorm(n_feats)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x (batch, channel, feature, time)
        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)
        x = self.layer_norm(x)
        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)


class ResidualCNN(nn.Module):
    """Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf
        except with layer norm instead of batch norm
    """

    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:
        super(ResidualCNN, self).__init__()
        
        self.conv_block = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel,
                stride=stride,
                padding=kernel // 2
            ),
            CNNLayerNorm(n_feats),
            nn.ReLU(),
            nn.Dropout(dropout, inplace=False),
            nn.Conv2d(
                in_channels=out_channels,
                out_channels=out_channels,
                kernel_size=kernel,
                stride=1,
                padding=kernel // 2
            ),
            CNNLayerNorm(n_feats)
        )

        # Residual connection: Adjust dimensions if necessary
        if in_channels != out_channels or stride != 1:
            self.residual_conv = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=1,
                stride=1,
                padding=0
            )
        else:
            self.residual_conv = nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x.shape = (batch, channel, feature, time)
        residual = self.residual_conv(x)
        x = self.conv_block(x)
        x += residual
        return x


class FeatureExtractor(nn.Module):
    def __init__(self, n_cnn_layers: int, rnn_dim: int,
                 n_feats: int, stride: int = 2, dropout: float = 0.1, channels: int = 32) -> None:
        super(FeatureExtractor, self).__init__()
        
        n_feats //= stride
        
        self.cnn = nn.Conv2d(
            in_channels=1,
            out_channels=channels,
            kernel_size=3,
            stride=stride,
            padding=3//2
        )

        # n residual cnn layers with filter size of 32
        residual_layers = [
            ResidualCNN(
                in_channels=channels,
                out_channels=channels,
                kernel=3,
                stride=1,
                dropout=dropout,
                n_feats=n_feats
            )
            for _ in range(n_cnn_layers)
        ]
        self.rescnn_layers = nn.Sequential(*residual_layers)

        self.fully_connected = nn.Sequential(
            nn.Linear(channels * n_feats, 4 * n_feats),
            nn.ReLU(),
            nn.Dropout(p=0.4, inplace=False),
            nn.Linear(4 * n_feats, rnn_dim),
            nn.ReLU(),
            nn.Dropout(p=0.2, inplace=False),
            nn.Linear(rnn_dim, rnn_dim)
        )
      

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x.shape = (batch, channel, feature, time)
        x = self.cnn(x)
        x = self.rescnn_layers(x)
        sizes = x.size()
        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)
        x = x.transpose(1, 2)  # (batch, time, feature)
        x = self.fully_connected(x)
        return x


class CTCDNN(nn.Module):

    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, 
                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:
        super(CTCDNN, self).__init__()
        
        self.feature_extractor = FeatureExtractor(
            n_cnn_layers,
            rnn_dim,
            n_feats,
            stride,
            dropout
        )
        
        self.intermediate_layers = nn.Sequential(
            nn.Linear(rnn_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.4, inplace=False),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.2, inplace=False),
            nn.Linear(256, rnn_dim),
        )

        self.classifier = nn.Linear(rnn_dim, n_class)

    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:
        x = self.feature_extractor(x)
        x = self.intermediate_layers(x)
        x = self.classifier(x)
        return x

class CTCRNN(nn.Module):

    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, 
                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:
        super(CTCRNN, self).__init__()

        self.feature_extractor = FeatureExtractor(
            n_cnn_layers,
            rnn_dim,
            n_feats,
            stride,
            dropout
        )

        self.intermediate_layers = nn.LSTM(
            input_size=rnn_dim,
            hidden_size=rnn_dim,
            num_layers=n_rnn_layers,
            batch_first=True,
            dropout=dropout if n_rnn_layers > 1 else 0,
            bidirectional=False,
            proj_size=n_class
        )
        
        self.classifier = nn.Identity()

    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:
        x = self.feature_extractor(x)
        x, _ = self.intermediate_layers(x)
        x = self.classifier(x)
        return x


class CTCBiRNN(nn.Module):

    def __init__(self, n_cnn_layers: int, n_rnn_layers: int, rnn_dim: int, n_class: int, 
                 n_feats: int, stride: int = 2, dropout: float = 0.1) -> None:
        super(CTCBiRNN, self).__init__()
        
        self.feature_extractor = FeatureExtractor(
            n_cnn_layers,
            rnn_dim,
            n_feats,
            stride,
            dropout
        )

        self.intermediate_layers = nn.LSTM(
            input_size=rnn_dim,
            hidden_size=rnn_dim,
            num_layers=n_rnn_layers,
            batch_first=True,
            dropout=dropout if n_rnn_layers > 1 else 0,
            bidirectional=True,
            proj_size=n_class
        )

        self.classifier = nn.Identity()


    def forward(self, x: torch.Tensor, input_lengths: torch.Tensor) -> torch.Tensor:
        x = self.feature_extractor(x)
        x, _ = self.intermediate_layers(x)
        x = self.classifier(x)
        return x
\Cell 30:
----------------------------------------
### Training and Evaluation Code
\Cell 31:
----------------------------------------
#!L

def train(model: nn.Module, device: str, train_loader: data.DataLoader, 
          criterion: nn.Module, optimizer: torch.optim.Optimizer, 
          scheduler: torch.optim.lr_scheduler, epoch: int) -> None:
    model.train()
    data_len = len(train_loader.dataset)
    for batch_idx, _data in enumerate(train_loader):
        spectrograms, labels, input_lengths, label_lengths = _data
        spectrograms, labels = spectrograms.to(device), labels.to(device)

        optimizer.zero_grad()

        output = model(spectrograms, input_lengths)  # (batch, time, n_class)
        output = F.log_softmax(output, dim=2)
        output = output.transpose(0, 1)  # (time, batch, n_class)

        loss = criterion(output, labels, input_lengths, label_lengths)
        loss.backward()

        optimizer.step()
        scheduler.step()
        if batch_idx % 100 == 0 or batch_idx == data_len:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(spectrograms), data_len,
                       100. * batch_idx / len(train_loader), loss.item()))
            wandb.log({'loss_train': loss.item()})
            
        del spectrograms, labels
        torch.cuda.empty_cache()


def test(model: nn.Module, device: str, test_loader: data.DataLoader, 
         criterion: nn.Module, epoch: int, decode: str = 'Greedy', lm: LanguageModel = None, save_path: str = None) -> None:
    print('Beginning eval...')
    model.eval()
    test_loss = 0
    test_cer, test_wer = [], []
    with torch.no_grad():
        start = time.time()
        for i, _data in enumerate(test_loader):
            spectrograms, labels, input_lengths, label_lengths = _data
            spectrograms, labels = spectrograms.to(device), labels.to(device)
            
            matrix = model(spectrograms, input_lengths)  # (batch, time, n_class)
            matrix = F.log_softmax(matrix, dim=2)
            probs = F.softmax(matrix,dim=2)
            matrix = matrix.transpose(0, 1)  # (time, batch, n_class)
                
            if i == 3:
                np.savetxt(f"{save_path}_matrix.txt", probs[0].cpu().numpy())
                np.savetxt(f"{save_path}_labels.txt", labels[0].cpu().numpy())

            loss = criterion(matrix, labels, input_lengths, label_lengths)
            test_loss += loss.item() / len(test_loader)
            
            decoded_preds = []
            if decode == 'Greedy':
                decoded_preds, decoded_targets = greedy_decoder(matrix.transpose(0, 1), labels, label_lengths)
            elif decode == 'BeamSearch':
                decoded_preds, decoded_targets = beam_search_decoder(probs, labels, label_lengths, input_lengths, lm=lm)
            
            for j in range(len(decoded_preds)):
                test_cer.append(utils.cer(decoded_targets[j], decoded_preds[j]))
                test_wer.append(utils.wer(decoded_targets[j], decoded_preds[j]))
            
            

    avg_cer = sum(test_cer) / len(test_cer)
    avg_wer = sum(test_wer) / len(test_wer)
    wandb.log({'loss_test': test_loss, 'avg_cer': avg_cer, 'avg_wer': avg_wer})
    print(
        'Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\n'.format(epoch, test_loss,
                                                                                                       avg_cer,
                                                                                                       avg_wer))
\Cell 32:
----------------------------------------
#!L
torch.manual_seed(7)
if torch.cuda.is_available():
    print('GPU found! üéâ')
    device = 'cuda'
else:
    print('Only CPU found! üíª')
    device = 'cpu'

verbose=False

# Hyperparameters for your model
hparams = {
    "n_cnn_layers": 5,
    "n_rnn_layers": 3,
    "rnn_dim": 128,
    "n_class": 29,
    "n_feats": 128,
    "stride": 2,
    "dropout": 0.2,
    "learning_rate": 1e-3,
    "batch_size":  64,
    "epochs": 20,
}

train_collate_fn = Collate(data_type='train')
test_collate_fn = Collate(data_type='test')

# Define Dataloyour training and test data loaders
kwargs = {'num_workers': 2, 'pin_memory': True} if device == 'cuda' else {}
train_loader = data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, collate_fn=train_collate_fn, **kwargs)

kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}
test_loader = data.DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False, collate_fn=test_collate_fn, **kwargs)
\Cell 33:
----------------------------------------
We recommend to use "Weights & Biases" for experiment logging. See their [documentation](https://docs.wandb.ai/) for detais.
\Cell 34:
----------------------------------------
## Compare different models: DNN, GRU/LSTM, bi-GRU/bi-LSTM (2 points)

Train and discuss differences in the different models. 

Compare performance of DNN, RNN and BiRNN models in terms of:
-  WER / CER 
-  Training time
-  Training stability 
-  Any other properties?
\Cell 35:
----------------------------------------
wandb.init(
    project="hw2-dlaudio", 
    group="DNN",
    config=hparams
)
\Cell 36:
----------------------------------------
# Train a non-recurrent model
ctc_dnn = CTCDNN(
    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],
    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout'])
ctc_dnn.to(device)

print(sum(p.numel() for p in ctc_dnn.parameters() if p.requires_grad))

optimizer = optim.Adam(ctc_dnn.parameters(), hparams['learning_rate'])
criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=hparams['learning_rate'],
    epochs=hparams['epochs'],
    steps_per_epoch=len(train_loader),
    div_factor=5,
    final_div_factor=10,
)

for epoch in tqdm(range(1, hparams['epochs'] + 1)):
    train(ctc_dnn, device, train_loader, criterion, optimizer, scheduler, epoch)
    utils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn_epoch{epoch}.tar')
    wandb.save(f'ctc_dnn_epoch{epoch}.tar')
    test(ctc_dnn, device, test_loader, criterion, epoch, save_path='dnn')

utils.save_checkpoint(ctc_dnn, checkpoint_name=f'ctc_dnn.tar')
\Cell 37:
----------------------------------------
wandb.init(
    project="hw2-dlaudio", 
    group="RNN",
    config=hparams
)
\Cell 38:
----------------------------------------
# Train a  recurrent model
ctc_rnn = CTCRNN(
    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],
    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']
).to(device)

optimizer = optim.Adam(ctc_rnn.parameters(), hparams['learning_rate'])
criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=hparams['learning_rate'],
    epochs=hparams['epochs'],
    steps_per_epoch=len(train_loader),
    div_factor=5,
    final_div_factor=10,
)

for epoch in tqdm(range(1, hparams['epochs'] + 1)):
    train(ctc_rnn, device, train_loader, criterion, optimizer, scheduler, epoch)
    utils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn_epoch{epoch}.tar')
    wandb.save(f'ctc_rnn_epoch{epoch}.tar')
    test(ctc_rnn, device, test_loader, criterion, epoch, save_path='rnn')
#     test(ctc_rnn, device, test_loader, criterion, epoch, save_path='rnn', decode='BeamSearch')

utils.save_checkpoint(ctc_rnn, checkpoint_name=f'ctc_rnn.tar')
\Cell 39:
----------------------------------------
wandb.init(
    project="hw2-dlaudio", 
    group="BiRNN",
    config=hparams
)
\Cell 40:
----------------------------------------
# Train a  recurrent model
hparams = {
    "n_cnn_layers": 4,
    "n_rnn_layers": 2,
    "rnn_dim": 128,
    "n_class": 29,
    "n_feats": 128,
    "stride": 2,
    "dropout": 0.2,
    "learning_rate": 1e-3,
    "batch_size":  64,
    "epochs": 20,
}

ctc_birnn = CTCBiRNN(
    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],
    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']
).to(device)

optimizer = optim.Adam(ctc_birnn.parameters(), hparams['learning_rate'])
criterion = nn.CTCLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=hparams['learning_rate'],
    epochs=hparams['epochs'],
    steps_per_epoch=len(train_loader),
    div_factor=5,
    final_div_factor=10,
)

for epoch in tqdm(range(1, hparams['epochs'] + 1)):
    train(ctc_birnn, device, train_loader, criterion, optimizer, scheduler, epoch)
    utils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn_epoch{epoch}.tar')
    wandb.save(f'ctc_birnn_epoch{epoch}.tar')
    test(ctc_birnn, device, test_loader, criterion, epoch, save_path='birnn')

utils.save_checkpoint(ctc_birnn, checkpoint_name=f'ctc_birnn.tar')
\Cell 41:
----------------------------------------
## Compare alignments (3 points)

In this section you should compare alignments obtained from different models (DNN / RNN / BiRNN). For example, you can show:

- Examples of alignments and their analysis. 
- Differencies in the properties of alignment distributions over the dataset. 
- Dynamic of alignments during training (from checkpoints). 
- Connection between alignments and model loss. 
- Which models use the most blanks and why?
\Cell 42:
----------------------------------------
# Some code to get you started.
dnn_matrix = np.loadtxt('dnn_matrix.txt').T
rnn_matrix = np.loadtxt('rnn_matrix.txt').T
birnn_matrix = np.loadtxt('birnn_matrix.txt').T

dnn_labels = np.loadtxt('dnn_labels.txt', dtype=np.int32)
dnn_labels = dnn_labels[dnn_labels != 0]
rnn_labels = np.loadtxt('rnn_labels.txt', dtype=np.int32)
rnn_labels = rnn_labels[rnn_labels != 0]
birnn_labels = np.loadtxt('birnn_labels.txt', dtype=np.int32)
birnn_labels = birnn_labels[birnn_labels != 0]

dnn_align = soft_alignment(dnn_labels, dnn_matrix)
rnn_align = soft_alignment(rnn_labels, rnn_matrix)
birnn_align = soft_alignment(birnn_labels, birnn_matrix)

f, ax = plt.subplots(3, 2, dpi=75, figsize=(15, 15))


im = ax[0,0].imshow(dnn_align, aspect='auto', interpolation='nearest')
ax[0,0].set_title("DNN Alignment")
ax[0,0].set_ylabel("Phonemes")
ax[0,0].set_xlabel("Time")
f.colorbar(im, ax=ax[0,0])

im = ax[0,1].imshow(np.log(dnn_align), aspect='auto', interpolation='nearest')
ax[0,1].set_title("DNN Alignment in log scale")
ax[0,1].set_ylabel("Phonemes")
ax[0,1].set_xlabel("Time")
f.colorbar(im, ax=ax[0,1])

im = ax[1,0].imshow(rnn_align, aspect='auto', interpolation='nearest')
ax[1,0].set_title("RNN Alignment")
ax[1,0].set_ylabel("Phonemes")
ax[1,0].set_xlabel("Time")
f.colorbar(im, ax=ax[1,0])

im = ax[1,1].imshow(np.log(rnn_align), aspect='auto', interpolation='nearest')
ax[1,1].set_title("RNN Alignment in log scale")
ax[1,1].set_ylabel("Phonemes")
ax[1,1].set_xlabel("Time")
f.colorbar(im, ax=ax[1,1])

im = ax[2,0].imshow(birnn_align, aspect='auto', interpolation='nearest')
ax[2,0].set_title("BiRNN Alignment")
ax[2,0].set_ylabel("Phonemes")
ax[2,0].set_xlabel("Time")
f.colorbar(im, ax=ax[2,0])

im = ax[2,1].imshow(np.log(birnn_align), aspect='auto', interpolation='nearest')
ax[2,1].set_title("BiRNN Alignment in log scale")
ax[2,1].set_ylabel("Phonemes")
ax[2,1].set_xlabel("Time")
f.colorbar(im, ax=ax[2,1])

plt.tight_layout()
\Cell 43:
----------------------------------------
## Modifications
\Cell 44:
----------------------------------------
### Download language model for better WER
\Cell 45:
----------------------------------------

\Cell 46:
----------------------------------------
### Exploit an advanced tokenizer
\Cell 47:
----------------------------------------

\Cell 48:
----------------------------------------
### Add more augmentations + reduce spectrogram's size
\Cell 49:
----------------------------------------

